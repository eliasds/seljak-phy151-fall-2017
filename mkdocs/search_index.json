{
    "docs": [
        {
            "location": "/",
            "text": "University of California, Berkeley, Department of Physics\n\n\nPHY151: Fall 2017\n\n\nNumerical methods, data science and statistics for physical sciences\n\n\nInstructor: Uro\u0161 Seljak, Campbell Hall 359, useljak@berkeley.edu\n\n\nTuesday, Thursday 9:30-11 AM, 325 LeConte Hall\n\n\nClass Number: 46359\n\n\nCourse Syllabus\n\n\nLearning goals\n:\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis concepts that are often encountered in real world physical science applications. \n\n\nTarget audience\n:\ntarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments.\n\n\nCourse structure\n:\neach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. This will be followed by a weekly python based homework assignment,\napplying a subset of the methods to physical science based applications. There will be a one hour discussion of the lecture material and homeworks.\n\n\nPrerequsites\n:\nUndergraduate students: PHY7 or PHY5 series, \nbasic introduction to Python programming at the level of PHY77 or permission from instructor. \nSome knowledge of analytic mechanics and statistical physics at the level of PHY105 and PHY112 will be assumed. Graduate students: equivalent of PHY105 and 112, and basic introduction to Python programming, or permission from instructor. \n\n\nGrades\n: 30% final project, 70% homeworks.\n\n\nWeekly Syllabus\n\n\nFunction integration\n: trapezoidal, Simpson, Romberg, gaussian quadratures\n\n\nSpecial functions, function evaluations and derivatives\n: series, recurrence relations, derivatives, Gamma, Bessel, spherical harmonics\n\n\nLinear algebra and eigensystems\n: Gauss elimination, LU and Cholesky decomposition, singular value decomposition, sparse algebra, matrix diagonalization, principal component analysis\n\n\nNonlinear sets of equations and root finding\n: relaxation, bisection, Newton's method\n\n\nOptimization (minimization/maximization)\n: (stochastic) gradient descent, conjugate gradient (with \npreconditioner), Newton and quasi-Newton (BFGS)\n\n\nInterpolation and extrapolation of data\n: Polynomial, rational and spline interpolation, gaussian processes\n\n\nFourier transforms\n: FFT, convolutions, power spectrum and correlation function, optimal (Wiener) filtering, wavelets\n\n\nOrdinary differential equations\n: Euler, Runge Kutta, Bulirsch-Stoer, stiff equation solvers, leap-frog and symplectic integrators\n\n\nPartial differential equations\n: boundary value and initial value problems\n\n\nStatistics\n: Bayesian inference, priors and posteriors, maximum likelihood and maximum a posterior, linear and\nnonlinear model fitting of data, regularization, hierarchical models, probabilistic graphical models\n\n\nInformation theory, experiment design and error estimation\n: Fisher information matrix, (Shannon information) entropy, \nanalytic covariance matrix, Monte Carlo simulations, jackknife, bootstrap\n\n\nRandom processes and Bayesian statistics\n: random number generators, Monte Carlo integration with random and sub-random sequences,\nMetropolis-Hastings algorithm, Markov Chain Monte Carlo, Gibbs sampling, importance sampling, Hamiltonian Monte Carlo, simulated annealing\n\n\nClassification and inference with machine learning and Bayesian Statistics\n: Gaussian mixtures with expectation\nmaximization algorithm, Decision Tree-Based methods, Support Vector Machines, backpropagation neural network algorithms\n\n\nOther topics\n: symbolic algebra with sympy or mathematica, sorting and neighbor finding, latex with Overleaf, version control with git(hub), debugging with pdb, presentations with powerpoint or keynote.\n\n\nLiterature\n\n\n\n\n\n\nNumerical Recipes\n, by Press. W. etal\n\n\n\n\n\n\nInformation Theory, Inference and Learning Algorithms\n, by David MacKay,\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n, by James G. etal, \n\n\n\n\n\n\nA Survey of Computational Physics\n by Landau, R., Paez, M-J., Bordeianu, C.\n\n\n\n\n\n\nhttp://greenteapress.com/wp/think-bayes/\n\n\n\n\n\n\nhttps://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n\n\n\n\n\n\nComputational Physics by Mark Newman\n\n\n\n\n\n\nEffective Computation in Physics, by A. Scopatz and K. D. Huff\n\n\n\n\n\n\nOther resources will be provided according to the needs.\n\n\nSoftware\n\n\nWe will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Many of these are already\nimplemented in python libraries (scipy, numpy...), or you can call Numerical Recipes C++ routines (as well as other\nroutines) from python (see \nhttp://numerical.recipes\n for details). \n\n\nHomeworks and Final Exam\n\n\nweekly homeworks, Jupyter notebook in Python submissions.\n\n\nThroughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results.\nThe GSI will also examine code for proper commenting and style.\nIf the code does not produce appropriate results from the test cases, GSIs will also check for errors to award partial credit for students\u2019 efforts.\nThese manual examinations for errors may be more or less thorough depending on enrollment and availability of the GSI.\nThere will be a take home final exam covering the last few weeks of lectures, following the same structure as the homeworks.\n\n\nTake home exam may differ between undergraduate and graduate students.",
            "title": "Home"
        },
        {
            "location": "/#numerical-methods-data-science-and-statistics-for-physical-sciences",
            "text": "Instructor: Uro\u0161 Seljak, Campbell Hall 359, useljak@berkeley.edu  Tuesday, Thursday 9:30-11 AM, 325 LeConte Hall  Class Number: 46359",
            "title": "Numerical methods, data science and statistics for physical sciences"
        },
        {
            "location": "/#course-syllabus",
            "text": "Learning goals :\nThe goals of the course is to get acquainted with modern computational methods\nused in physical sciences, including numerical analysis methods, data science and statistics. \nWe will introduce a number of concepts that are useful in physical sciences at varying depth levels. We will cover main numerical methods used in physical sciences. Most of the statistical concepts will be Bayesian, \nemphasizing the concepts that have a connection to physical sciences, such as classical and statistical mechanics. We will focus on data science and data analysis concepts that are often encountered in real world physical science applications.   Target audience :\ntarget student population are upper division undergraduates from physical science departments, as well as beginning graduate students from the same departments.  Course structure :\neach week there will be a set of 3 hour lectures discussing theoretical and practical underpinnings of the specific topic,\ntogether with its most common applications in physical sciences. This will be followed by a weekly python based homework assignment,\napplying a subset of the methods to physical science based applications. There will be a one hour discussion of the lecture material and homeworks.  Prerequsites :\nUndergraduate students: PHY7 or PHY5 series, \nbasic introduction to Python programming at the level of PHY77 or permission from instructor. \nSome knowledge of analytic mechanics and statistical physics at the level of PHY105 and PHY112 will be assumed. Graduate students: equivalent of PHY105 and 112, and basic introduction to Python programming, or permission from instructor.   Grades : 30% final project, 70% homeworks.",
            "title": "Course Syllabus"
        },
        {
            "location": "/#weekly-syllabus",
            "text": "Function integration : trapezoidal, Simpson, Romberg, gaussian quadratures  Special functions, function evaluations and derivatives : series, recurrence relations, derivatives, Gamma, Bessel, spherical harmonics  Linear algebra and eigensystems : Gauss elimination, LU and Cholesky decomposition, singular value decomposition, sparse algebra, matrix diagonalization, principal component analysis  Nonlinear sets of equations and root finding : relaxation, bisection, Newton's method  Optimization (minimization/maximization) : (stochastic) gradient descent, conjugate gradient (with \npreconditioner), Newton and quasi-Newton (BFGS)  Interpolation and extrapolation of data : Polynomial, rational and spline interpolation, gaussian processes  Fourier transforms : FFT, convolutions, power spectrum and correlation function, optimal (Wiener) filtering, wavelets  Ordinary differential equations : Euler, Runge Kutta, Bulirsch-Stoer, stiff equation solvers, leap-frog and symplectic integrators  Partial differential equations : boundary value and initial value problems  Statistics : Bayesian inference, priors and posteriors, maximum likelihood and maximum a posterior, linear and\nnonlinear model fitting of data, regularization, hierarchical models, probabilistic graphical models  Information theory, experiment design and error estimation : Fisher information matrix, (Shannon information) entropy, \nanalytic covariance matrix, Monte Carlo simulations, jackknife, bootstrap  Random processes and Bayesian statistics : random number generators, Monte Carlo integration with random and sub-random sequences,\nMetropolis-Hastings algorithm, Markov Chain Monte Carlo, Gibbs sampling, importance sampling, Hamiltonian Monte Carlo, simulated annealing  Classification and inference with machine learning and Bayesian Statistics : Gaussian mixtures with expectation\nmaximization algorithm, Decision Tree-Based methods, Support Vector Machines, backpropagation neural network algorithms  Other topics : symbolic algebra with sympy or mathematica, sorting and neighbor finding, latex with Overleaf, version control with git(hub), debugging with pdb, presentations with powerpoint or keynote.",
            "title": "Weekly Syllabus"
        },
        {
            "location": "/#literature",
            "text": "Numerical Recipes , by Press. W. etal    Information Theory, Inference and Learning Algorithms , by David MacKay,    An Introduction to Statistical Learning , by James G. etal,     A Survey of Computational Physics  by Landau, R., Paez, M-J., Bordeianu, C.    http://greenteapress.com/wp/think-bayes/    https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers    Computational Physics by Mark Newman    Effective Computation in Physics, by A. Scopatz and K. D. Huff    Other resources will be provided according to the needs.",
            "title": "Literature"
        },
        {
            "location": "/#software",
            "text": "We will use Python Jupyter hub environment. You are expected to use existing numerical analysis routines and not write your own. Many of these are already\nimplemented in python libraries (scipy, numpy...), or you can call Numerical Recipes C++ routines (as well as other\nroutines) from python (see  http://numerical.recipes  for details).",
            "title": "Software"
        },
        {
            "location": "/#homeworks-and-final-exam",
            "text": "weekly homeworks, Jupyter notebook in Python submissions.  Throughout the course, students will complete and electronically submit one homework assignment (code) per week.\nThis code will be run on test cases to check if it produces appropriate results.\nThe GSI will also examine code for proper commenting and style.\nIf the code does not produce appropriate results from the test cases, GSIs will also check for errors to award partial credit for students\u2019 efforts.\nThese manual examinations for errors may be more or less thorough depending on enrollment and availability of the GSI.\nThere will be a take home final exam covering the last few weeks of lectures, following the same structure as the homeworks.  Take home exam may differ between undergraduate and graduate students.",
            "title": "Homeworks and Final Exam"
        },
        {
            "location": "/homeworks/",
            "text": "Homeworks\n\n\nHome works assignments will be posted here..\n\n\n\n\nDate, Link to datahub repo.",
            "title": "Homeworks"
        },
        {
            "location": "/homeworks/#homeworks",
            "text": "Home works assignments will be posted here..   Date, Link to datahub repo.",
            "title": "Homeworks"
        },
        {
            "location": "/math/",
            "text": "Example adding math to pages.\n\n\nWhen \na \\ne 0\n, there are two solutions to \nax^2 + bx + c = 0\n and they are\n\nx = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.\n\n\n\n\n\n\n\n c = 193",
            "title": "Math"
        }
    ]
}